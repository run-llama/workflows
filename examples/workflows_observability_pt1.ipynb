{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sunqHvPG4el9"
   },
   "source": [
    "# Workflows Observability - Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vx09YKJjycji"
   },
   "source": [
    "\n",
    "## Use native instrumentation from LlamaIndex + OpenTelemetry to fine-grain tracing in your code!\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s1yQ1Rmr4I6Q"
   },
   "source": [
    "In this notebook, we will go through an example of how to use instrumentation natively implemented in `llama-index` (combined with OpenTelemetry) to define costum span and events within your code. Before we get started:\n",
    "\n",
    "\n",
    "⭐ Don'f forget to star the `llama-index-workflows` [GitHub repo](https://github.com/run-llama/workflows-py)\n",
    "\n",
    "🦙☁ Register to [LlamaCloud](https://cloud.llamaindex.ai) not to miss out on all our awesome products\n",
    "\n",
    "If you have feedback, questions, issues, or you just want to follow us not to miss out on any news, please find us on:\n",
    "\n",
    "[![GitHub](https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&logo=github&logoColor=white)](https://github.com/run-llama/)\n",
    "[![Discord](https://img.shields.io/badge/Discord-%235865F2.svg?style=for-the-badge&logo=discord&logoColor=white)](https://discord.com/invite/eN6D2HQ4aX)\n",
    "[![X](https://img.shields.io/badge/@llama__index-%23000000.svg?style=for-the-badge&logo=X&logoColor=white)](https://x.com/@llama_index)\n",
    "[![Bluesky](https://img.shields.io/badge/Bluesky-0285FF?style=for-the-badge&logo=Bluesky&logoColor=white)](https://bsky.app/profile/llamaindex.bsky.social)\n",
    "[![LinkedIn](https://img.shields.io/badge/linkedin-%230077B5.svg?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/company/llamaindex/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PCIOKSNJzwJB"
   },
   "source": [
    "## 1. Setting up\n",
    "\n",
    "Before diving deep into all of this, let's install all the needed dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9yQ8Z5zByLuj",
    "outputId": "667f07b7-1cfb-45fb-e4f5-624d177fb8dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/7.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/7.6 MB\u001b[0m \u001b[31m114.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m6.8/7.6 MB\u001b[0m \u001b[31m96.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m96.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/65.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install -q llama-index-workflows llama-index-instrumentation llama-index-llms-openai llama-index-observability-otel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cyHfFgXP0rzA"
   },
   "source": [
    "## 2. Experiment with instrumentation\n",
    "\n",
    "Let's now play around with `llama-index` dispatcher and see how we can make it work.\n",
    "\n",
    "Let's start by initializing it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "IRGWgr4t1EXl"
   },
   "outputs": [],
   "source": [
    "from llama_index_instrumentation import get_dispatcher\n",
    "\n",
    "dispatcher = get_dispatcher()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YAnMutOL1xa6"
   },
   "source": [
    "Now we can use the `@dispatcher.span` decorator on a function that we defined to emit spans (containers for events) and use `dispatcher.event` to emit and event (we can define custom events by subclassing the `BaseEvent` class):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "hprqOiV81w25"
   },
   "outputs": [],
   "source": [
    "from llama_index_instrumentation.base import BaseEvent\n",
    "\n",
    "\n",
    "class ExampleEvent(BaseEvent):\n",
    "    data: str\n",
    "\n",
    "\n",
    "@dispatcher.span\n",
    "def example_fn(data: str) -> None:\n",
    "    dispatcher.event(ExampleEvent(data=data))\n",
    "    s = \"This are example string data: \" + data\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iDrSR1Xw6Wvn"
   },
   "source": [
    "## 3. Add OpenTelemetry\n",
    "\n",
    "We can now add OpenTelemetry so that we can export all our span and events as ordered traces.\n",
    "We will be using the LlamaIndex integration for that, `llama-index-observability-otel`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6dauMQp6Jt9r"
   },
   "source": [
    "We start by defining a custom `SpanExporter` that can write all our traces to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "lKIvFShXD4qz"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import json\n",
    "\n",
    "from llama_index.observability.otel import LlamaIndexOpenTelemetry\n",
    "from opentelemetry.sdk.trace.export import SpanExporter, SpanExportResult\n",
    "from opentelemetry.sdk.trace import ReadableSpan\n",
    "from typing import Optional, Callable, Sequence\n",
    "from os import linesep\n",
    "\n",
    "\n",
    "class FileSpanExporter(SpanExporter):\n",
    "    \"\"\"Implementation of :class:`SpanExporter` that prints spans to the\n",
    "    console.\n",
    "\n",
    "    This class can be used for diagnostic purposes. It prints the exported\n",
    "    spans to the console STDOUT.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        service_name: str | None = None,\n",
    "        file_path: Optional[os.PathLike[str]] = None,\n",
    "        formatter: Callable[[ReadableSpan], str] = lambda span: json.dumps(\n",
    "            json.loads(span.to_json())\n",
    "        )\n",
    "        + linesep,\n",
    "    ):\n",
    "        if not file_path:\n",
    "            file_path = \"traces.json\"\n",
    "        if Path(file_path).exists():\n",
    "            raise ValueError(f\"File {file_path} already exists\")\n",
    "        self.file_path = file_path\n",
    "        self.formatter = formatter\n",
    "        self.service_name = service_name\n",
    "\n",
    "    def export(self, spans: Sequence[ReadableSpan]) -> SpanExportResult:\n",
    "        print(f\"Writing {len(spans)} spans to {self.file_path}\")\n",
    "        if Path(self.file_path).exists():\n",
    "            mode = \"a\"\n",
    "        else:\n",
    "            mode = \"w\"\n",
    "        with open(self.file_path, mode) as out:\n",
    "            for span in spans:\n",
    "                out.write(self.formatter(span))\n",
    "            out.flush()\n",
    "        return SpanExportResult.SUCCESS\n",
    "\n",
    "    def force_flush(self, timeout_millis: int = 30000) -> bool:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2k0mZcDO5lcy"
   },
   "source": [
    "It is important to notice that we are defining here a custom span exporter since it is an easier implementation for notebooks, but there are many exporting options detailed by OpenTelemetry in [this page](https://opentelemetry.io/docs/languages/python/exporters/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A4KJhJTuJ6MR"
   },
   "source": [
    "Now we can pass that to the instrumentation class as a span exporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "pLWJQSEZJ5fg"
   },
   "outputs": [],
   "source": [
    "se = FileSpanExporter(file_path=\"traces_example.json\")\n",
    "\n",
    "instrumentor = LlamaIndexOpenTelemetry(\n",
    "    span_exporter=se,\n",
    "    service_name_or_resource=\"example_service\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i57Ozf_rK9lz"
   },
   "source": [
    "And we can try and see how events are registered just by calling an LLM, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vWZxGUpVOFd7",
    "outputId": "8b1789d6-5dd3-4734-b009-0d62af66b022"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "··········\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ZRWbHkAjOXJF"
   },
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "h9vLcbwdK7Ae",
    "outputId": "e4a4e1c6-2940-4889-e2d4-7d69631255a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I’m ChatGPT, an AI language model created by OpenAI. I’m here to help answer your questions, have conversations, and assist with a wide range of topics. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "instrumentor.start_registering()\n",
    "\n",
    "res = llm.complete(\"Hello there, who are you?\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "qdg-bqKFLMn7",
    "outputId": "da9ce73e-b58d-4220-cab7-478d0b4f9573"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"name\": \"OpenAI.complete-cf3c68ee-aaa9-47e0-8a91-def8587a3208\",\n",
      "    \"context\": {\n",
      "        \"trace_id\": \"0x6e21be858a3dcc388d22be4f84aa0724\",\n",
      "        \"span_id\": \"0xdf55676fe20a4509\",\n",
      "        \"trace_state\": \"[]\"\n",
      "    },\n",
      "    \"kind\": \"SpanKind.INTERNAL\",\n",
      "    \"parent_id\": null,\n",
      "    \"start_time\": \"2025-07-04T14:47:56.243684Z\",\n",
      "    \"end_time\": \"2025-07-04T14:47:58.280424Z\",\n",
      "    \"status\": {\n",
      "        \"status_code\": \"OK\"\n",
      "    },\n",
      "    \"attributes\": {},\n",
      "    \"events\": [\n",
      "        {\n",
      "            \"name\": \"LLMCompletionStartEvent\",\n",
      "            \"timestamp\": \"2025-07-04T14:47:58.280382Z\",\n",
      "            \"attributes\": {\n",
      "                \"id_\": \"7c807c9e-ee8c-4ba9-98b0-2700494ffa8d\",\n",
      "                \"span_id\": \"OpenAI.complete-cf3c68ee-aaa9-47e0-8a91-def8587a3208\",\n",
      "                \"prompt\": \"Hello there, who are you?\",\n",
      "                \"class_name\": \"LLMCompletionStartEvent\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"LLMCompletionEndEvent\",\n",
      "            \"timestamp\": \"2025-07-04T14:47:58.280406Z\",\n",
      "            \"attributes\": {\n",
      "                \"id_\": \"6c7e889b-781a-4954-9ed0-d06bd5b38b16\",\n",
      "                \"span_id\": \"OpenAI.complete-cf3c68ee-aaa9-47e0-8a91-def8587a3208\",\n",
      "                \"prompt\": \"Hello there, who are you?\",\n",
      "                \"class_name\": \"LLMCompletionEndEvent\"\n",
      "            }\n",
      "        }\n",
      "    ],\n",
      "    \"links\": [],\n",
      "    \"resource\": {\n",
      "        \"attributes\": {\n",
      "            \"service.name\": \"example_service\"\n",
      "        },\n",
      "        \"schema_url\": \"\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open(\"traces_example.json\") as f:\n",
    "    lines = f.readlines()\n",
    "    print(json.dumps(json.loads(lines[0]), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y3FvDOByPKyr"
   },
   "source": [
    "## 4. Instrument a workflow\n",
    "\n",
    "Now that we know:\n",
    "1. How to dispatch span and events\n",
    "2. How to register those events as OpenTelemetry traces\n",
    "\n",
    "It's time to use this knowledge to build and instrument a workflow!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AfjtWqy3Qwbn"
   },
   "source": [
    "The workflow that we want to build is very simple, and involves using OpenAI to analyze some short novels and breaking them down in different parts such as introduction, development of the plot and conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OcxSqxE7Vkx0"
   },
   "source": [
    "### 4.1 Define custom events\n",
    "\n",
    "The first thing that we need to do is to define the custom event that we will us throughout our framework.\n",
    "\n",
    "We can do it by subclassing the general `Event` classes that the `llama-index-workflows` package provides us with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5wJ_xQ-dVacH"
   },
   "outputs": [],
   "source": [
    "from workflows.events import StartEvent, Event, StopEvent\n",
    "\n",
    "\n",
    "class InputTextEvent(StartEvent):\n",
    "    input_text: str\n",
    "\n",
    "\n",
    "class AnalyzedTextEvent(StopEvent):\n",
    "    introduction: str\n",
    "    development: str\n",
    "    conclusion: str\n",
    "\n",
    "\n",
    "class ProgressEvent(Event):\n",
    "    msg: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gzoPnxIpn_JJ"
   },
   "source": [
    "### 4.2 Define custom resources\n",
    "\n",
    "[Resources](https://docs.llamaindex.ai/en/stable/understanding/workflows/resources/) are a way of performing dependency injection in workflow steps.\n",
    "\n",
    "We will need just one resource, i.e. an LLM able to produce a structured output that aligns with the text analysis we want to perform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x0di0zk4pUwY"
   },
   "source": [
    "We need to specify a schema for the structured output first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "FebjynuqiZMu"
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class TextAnalysis(BaseModel):\n",
    "    introduction: str = Field(description=\"Introduction of the novel\")\n",
    "    development: str = Field(description=\"Development of the novel\")\n",
    "    conclusion: str = Field(description=\"Conclusion of the novel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AyYTvTsq76JW"
   },
   "source": [
    "Let's then initialize the LLM that we used above as a structured LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "-Zkt11QY8FAJ"
   },
   "outputs": [],
   "source": [
    "struct_llm = llm.as_structured_llm(TextAnalysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ilPjdAXlzhEZ"
   },
   "source": [
    "Let's now define the function that would get us our LLM as resource:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "oRiKpN1QzrD3"
   },
   "outputs": [],
   "source": [
    "from llama_index.core.llms.structured_llm import StructuredLLM\n",
    "\n",
    "\n",
    "async def get_llm(*args, **kwargs) -> StructuredLLM:\n",
    "    return struct_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KwxmTX030oHH"
   },
   "source": [
    "### 4.3 Create the workflow\n",
    "\n",
    "Finally, after defining events and resources, we can create our workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "dauCQq1a8uTp"
   },
   "outputs": [],
   "source": [
    "from workflows import Workflow, step, Context\n",
    "from workflows.resource import Resource\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from typing import Annotated\n",
    "\n",
    "\n",
    "class TextAnalysisWorkflow(Workflow):\n",
    "    @step\n",
    "    async def analyze_text(\n",
    "        self,\n",
    "        event: InputTextEvent,\n",
    "        ctx: Context,\n",
    "        llm: Annotated[StructuredLLM, Resource(get_llm)],\n",
    "    ) -> AnalyzedTextEvent:\n",
    "        response = await llm.achat(\n",
    "            messages=[\n",
    "                ChatMessage(\n",
    "                    role=\"user\",\n",
    "                    content=f\"Analyze the following text: {event.input_text}\",\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        ctx.write_event_to_stream(ProgressEvent(msg=\"Text analyzed successfully\"))\n",
    "        response_json = json.loads(response.message.content)\n",
    "        return AnalyzedTextEvent(**response_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BVtoh_cvBCg3"
   },
   "source": [
    "Now we will run the workflow as-is, an you will already see that it produces OpenTelemtry traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k1yUKH2YBcYE",
    "outputId": "d13f0e30-d709-44f9-a472-56aec348e7d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  1297  100  1297    0     0  12082      0 --:--:-- --:--:-- --:--:-- 12121\n"
     ]
    }
   ],
   "source": [
    "# first let's get some data\n",
    "! curl https://raw.githubusercontent.com/run-llama/workflows-observability-support-data/main/data/short-stories/short_story.txt > short_story.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137
    },
    "id": "ZzlU5fUdBqQ3",
    "outputId": "75c94e85-d878-4ed6-e562-d977196d8c4d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Clara wandered through the old town library, seeking quiet more than books. On a dusty shelf near the back, she pulled down a forgotten novel, its spine cracked and pages yellowed. As she flipped it open, a folded letter slipped out and fluttered to the floor. Curious, she picked it up and read the faded ink: a heartfelt message from a soldier named James to someone named Eleanor, dated 1943. He spoke of love, hope, and his promise to return from war.\\n\\nUnable to shake the letter from her mind, Clara began digging into the town’s history. She scoured archives, interviewed elderly residents, and traced records through the war memorials. Piece by piece, the story came together: James had never made it home. Eleanor had waited, never knowing why he stopped writing. After weeks of searching, Clara finally found her—Eleanor, now 98, living in a quiet nursing home just outside town.\\n\\nWhen Clara placed the letter in Eleanor’s hands, the old woman wept. Her voice trembled as she read James’s words for the first time. “I always thought he would have written,” she whispered. The letter, lost for decades, had found its way home. As Clara left the nursing home, heart full, she knew this was only the beginning—of her own journey into forgotten stories and voices long silenced.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's read these data\n",
    "\n",
    "with open(\"short_story.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Crk4r4WSARIm"
   },
   "source": [
    "We will now use a different instrumentation object, but be careful: you might need to restart the notebook session and re-run all cells apart from the one where we instantiate and start the `instrumentor` object to make it work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "4BgHaqhGCKUX"
   },
   "outputs": [],
   "source": [
    "# let's export the spans to a different files, and then start the instrumentation\n",
    "se_1 = FileSpanExporter(file_path=\"workflow_1.json\")\n",
    "\n",
    "instrumentor_1 = LlamaIndexOpenTelemetry(\n",
    "    span_exporter=se_1,\n",
    "    service_name_or_resource=\"tracing.a.workflow.1\",\n",
    ")\n",
    "\n",
    "instrumentor_1.start_registering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cOYzx6hSBJ_J",
    "outputId": "97fc1d4b-5cb7-49f6-9537-f5797b73281f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 1 spans to workflow_1.json\n",
      "Text analyzed successfully\n"
     ]
    }
   ],
   "source": [
    "wf = TextAnalysisWorkflow(timeout=800)\n",
    "\n",
    "handler = wf.run(start_event=InputTextEvent(input_text=text))\n",
    "async for ev in handler.stream_events():\n",
    "    if isinstance(ev, ProgressEvent):\n",
    "        print(ev.msg, flush=True)\n",
    "\n",
    "result = await handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H_2kI8U7Dm9b",
    "outputId": "eb910aaf-2455-43d7-d372-beb9ea011215"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction\n",
      "\n",
      " Clara wandered through the old town library, seeking quiet more than books. On a dusty shelf near the back, she pulled down a forgotten novel, its spine cracked and pages yellowed. As she flipped it open, a folded letter slipped out and fluttered to the floor. Curious, she picked it up and read the faded ink: a heartfelt message from a soldier named James to someone named Eleanor, dated 1943. He spoke of love, hope, and his promise to return from war.\n",
      "\n",
      "--\n",
      "\n",
      "Development\n",
      "\n",
      " Unable to shake the letter from her mind, Clara began digging into the town’s history. She scoured archives, interviewed elderly residents, and traced records through the war memorials. Piece by piece, the story came together: James had never made it home. Eleanor had waited, never knowing why he stopped writing. After weeks of searching, Clara finally found her—Eleanor, now 98, living in a quiet nursing home just outside town.\n",
      "\n",
      "--\n",
      "\n",
      "Conclusion\n",
      "\n",
      " When Clara placed the letter in Eleanor’s hands, the old woman wept. Her voice trembled as she read James’s words for the first time. “I always thought he would have written,” she whispered. The letter, lost for decades, had found its way home. As Clara left the nursing home, heart full, she knew this was only the beginning—of her own journey into forgotten stories and voices long silenced.\n"
     ]
    }
   ],
   "source": [
    "print(\"Introduction\\n\\n\", result.introduction)\n",
    "print(\"\\n--\\n\")\n",
    "print(\"Development\\n\\n\", result.development)\n",
    "print(\"\\n--\\n\")\n",
    "print(\"Conclusion\\n\\n\", result.conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cmiYCDstDzKz"
   },
   "source": [
    "As you can see from the output of the cell where we executed the workflow, our OpenTelemtry instrumentation has wrote several spans to the traces file. We can confirm by printing some of the records out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "et8hRebkFDlH"
   },
   "outputs": [],
   "source": [
    "with open(\"workflow_1.json\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines[-5:]:\n",
    "        print(json.dumps(json.loads(line), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQVUbgEBFRwt"
   },
   "source": [
    "You could also create a workflow that has customized events, in our case that would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "aR3C78CNF-iI"
   },
   "outputs": [],
   "source": [
    "# define base events\n",
    "class TextAnalyzedWorkflowEvent(BaseEvent):\n",
    "    pass\n",
    "\n",
    "\n",
    "class InputTextWorkflowEvent(BaseEvent):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "vUZdNOpyFq5M"
   },
   "outputs": [],
   "source": [
    "# create an instrumented workflow\n",
    "class TextAnalysisWorkflowInst(Workflow):\n",
    "    @step\n",
    "    @dispatcher.span\n",
    "    async def analyze_text(\n",
    "        self,\n",
    "        event: InputTextEvent,\n",
    "        ctx: Context,\n",
    "        llm: Annotated[StructuredLLM, Resource(get_llm)],\n",
    "    ) -> AnalyzedTextEvent:\n",
    "        dispatcher.event(InputTextWorkflowEvent())\n",
    "        response = await llm.achat(\n",
    "            messages=[\n",
    "                ChatMessage(\n",
    "                    role=\"user\",\n",
    "                    content=f\"Analyze the following text: {event.input_text}\",\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        ctx.write_event_to_stream(ProgressEvent(msg=\"Text analyzed successfully\"))\n",
    "        dispatcher.event(TextAnalyzedWorkflowEvent())\n",
    "        response_json = json.loads(response.message.content)\n",
    "        return AnalyzedTextEvent(**response_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mazDgFljHY5J",
    "outputId": "f392fdb7-3bf6-45f5-cf93-5c278af02e21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 1 spans to workflow_1.json\n",
      "Text analyzed successfully\n"
     ]
    }
   ],
   "source": [
    "# Let's re-run the custom instrumented workflow\n",
    "\n",
    "wf = TextAnalysisWorkflowInst(timeout=800)\n",
    "\n",
    "handler = wf.run(start_event=InputTextEvent(input_text=text))\n",
    "async for ev in handler.stream_events():\n",
    "    if isinstance(ev, ProgressEvent):\n",
    "        print(ev.msg, flush=True)\n",
    "\n",
    "result = await handler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iPx3T4j5IiaO"
   },
   "source": [
    "If we now print workflow_1.json, we will see that the tracer has registered also our custom spans:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "sMf5cjYVIsif",
    "outputId": "f1bfa437-7feb-4264-97c6-52ba0b3e8289"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"name\": \"TextAnalysisWorkflowInst.analyze_text-be545ce0-af78-447f-8591-013b96e308a0\",\n",
      "    \"context\": {\n",
      "        \"trace_id\": \"0x49e3f4bafda0bd63225f6909a001e9c7\",\n",
      "        \"span_id\": \"0x051f6d41a6cb5dc5\",\n",
      "        \"trace_state\": \"[]\"\n",
      "    },\n",
      "    \"kind\": \"SpanKind.INTERNAL\",\n",
      "    \"parent_id\": \"0x161da23ed55e680c\",\n",
      "    \"start_time\": \"2025-07-04T15:21:08.095877Z\",\n",
      "    \"end_time\": \"2025-07-04T15:21:10.429656Z\",\n",
      "    \"status\": {\n",
      "        \"status_code\": \"OK\"\n",
      "    },\n",
      "    \"attributes\": {},\n",
      "    \"events\": [\n",
      "        {\n",
      "            \"name\": \"BaseEvent\",\n",
      "            \"timestamp\": \"2025-07-04T15:21:10.429625Z\",\n",
      "            \"attributes\": {\n",
      "                \"id_\": \"c64b52d1-9b84-4553-bb75-ef282b93edc1\",\n",
      "                \"span_id\": \"TextAnalysisWorkflowInst.analyze_text-be545ce0-af78-447f-8591-013b96e308a0\",\n",
      "                \"class_name\": \"BaseEvent\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"BaseEvent\",\n",
      "            \"timestamp\": \"2025-07-04T15:21:10.429644Z\",\n",
      "            \"attributes\": {\n",
      "                \"id_\": \"8522bcb3-4e82-46e1-9340-1e1e55741bc9\",\n",
      "                \"span_id\": \"TextAnalysisWorkflowInst.analyze_text-be545ce0-af78-447f-8591-013b96e308a0\",\n",
      "                \"class_name\": \"BaseEvent\"\n",
      "            }\n",
      "        }\n",
      "    ],\n",
      "    \"links\": [],\n",
      "    \"resource\": {\n",
      "        \"attributes\": {\n",
      "            \"service.name\": \"tracing.a.workflow.1\"\n",
      "        },\n",
      "        \"schema_url\": \"\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open(\"workflow_1.json\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    print(json.dumps(json.loads(lines[-3]), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NImiRjEsCI_v"
   },
   "source": [
    "As you can see, these two `BaseEvent` instances are exactly our custom events!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MIBb_qieC7ft"
   },
   "source": [
    "This is all for Part 1, in Part 2 we will be diving deeper into a more complex workflow, with many more events and more room for customization... See you there!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
